{"id":"automated_plan_reviser_pro-0fm","title":"Unit Tests: Update and Download Functions (check_for_updates, cmd_update, download validation)","description":"# Task: Unit Tests for Update Functions\n\n## Objective\nTest self-update functionality with REAL network behavior (controlled endpoints).\n\n## Functions to Test\n\n### 1. check_for_updates() - Daily Update Check\n```bash\n# Test cases:\n- APR_CHECK_UPDATES not set → skip check\n- APR_CHECK_UPDATES=1 → perform check\n- Already checked today (timestamp file) → skip\n- Older than 24h → check again\n- Remote version newer → show update message\n- Remote version same → no message\n- Remote version older → no message\n- Network timeout → graceful failure, no crash\n\n# Timestamp file:\n- Created in $APR_HOME/.last_update_check\n- Contains Unix timestamp\n- Updated after each check\n```\n\n### 2. cmd_update() - Self-Update Command\n```bash\n# Test with mock HTTP server or controlled URLs:\n# Note: We can use a local HTTP server for \"real\" testing\n\n# Version checking:\n- Fetch VERSION file\n- Parse remote version correctly\n- Compare with current VERSION\n- Already up to date message\n- Newer version available message\n\n# Download and verification:\n- Download apr script\n- Verify bash shebang (^#!.*bash regex)\n- Checksum verification when available\n- Checksum format: \"hash\" or \"hash  filename\"\n- Handle missing checksum gracefully\n\n# Installation:\n- Detect installation path from BASH_SOURCE\n- Handle writable path (mv)\n- Handle non-writable path (sudo mv)\n- Permission preservation\n\n# Error cases:\n- Network error → EXIT_NETWORK_ERROR\n- Download failure → EXIT_UPDATE_ERROR\n- Invalid shebang → EXIT_UPDATE_ERROR\n- Checksum mismatch → EXIT_UPDATE_ERROR\n- Permission denied → error message\n\n# Cleanup:\n- Temp directory created via APR_TEMP_DIR\n- Temp directory cleaned on success\n- Temp directory cleaned on failure\n```\n\n### 3. Shebang Validation\n```bash\n# Test cases:\n- \"#!/bin/bash\" → valid\n- \"#!/usr/bin/env bash\" → valid\n- \"#!/usr/bin/bash\" → valid\n- \"#!/bin/sh\" → invalid\n- \"#!/usr/bin/env sh\" → invalid\n- Empty file → invalid\n- Binary file → invalid\n- Script with BOM → handle gracefully\n```\n\n## Testing Approach\n```bash\n# Use local HTTP server for controlled testing:\n# Start server with test files\npython3 -m http.server 8888 --directory tests/fixtures/update/ \u0026\nexport VERSION_URL=\"http://localhost:8888/VERSION\"\nexport RELEASES_URL=\"http://localhost:8888/releases\"\n\n# Test fixtures:\ntests/fixtures/update/\n├── VERSION           # Contains \"2.0.0\"\n├── releases/\n│   └── download/\n│       └── v2.0.0/\n│           ├── apr       # Valid bash script\n│           └── apr.sha256\n```\n\n## Acceptance Criteria\n- [ ] check_for_updates respects opt-in flag\n- [ ] Daily check throttling works\n- [ ] cmd_update downloads and validates correctly\n- [ ] Shebang validation catches invalid scripts\n- [ ] Checksum verification works (both formats)\n- [ ] Error handling covers all failure modes\n- [ ] Cleanup happens on all exit paths","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T20:08:59.666220376-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:08:59.666220376-05:00"}
{"id":"automated_plan_reviser_pro-2my","title":"Integration Tests: Run Command (dry-run, render, preflight)","description":"# Task: Integration Tests for Run Command\n\n## Objective\nTest the run command comprehensively using dry-run and render modes (no actual Oracle calls).\n\n## Test Scenarios\n\n### 1. Dry Run Mode (--dry-run)\n```bash\n# Setup: Create complete workflow config\n# Test:\napr run 1 --dry-run\n\n# Verify output shows:\n- Oracle command that would be executed\n- Model selection\n- File arguments\n- Slug format\n- Output file path\n- All flags passed correctly\n```\n\n### 2. Render Mode (--render)\n```bash\n# Test:\napr run 1 --render\n\n# Verify:\n- Prompt content rendered to stdout\n- Files would be included\n- No Oracle process started\n```\n\n### 3. Render with Copy (--render --copy)\n```bash\n# Test:\napr run 1 --render --copy\n\n# Verify clipboard integration (if available)\n```\n\n### 4. Preflight Validation\n```bash\n# Test successful preflight:\napr run 1 --dry-run\n# Verify: \"All pre-flight checks passed\"\n\n# Test failed preflight (missing file):\nrm README.md\napr run 1 --dry-run\n# Verify: Error message, non-zero exit\n\n# Test skip preflight:\napr run 1 --dry-run --no-preflight\n# Verify: Skips checks, continues\n```\n\n### 5. Include Implementation Flag\n```bash\n# Test --include-impl:\napr run 1 --dry-run --include-impl\n\n# Verify:\n- Implementation file in command args\n- Slug includes \"-with-impl\"\n- Correct template loaded\n```\n\n### 6. Workflow Selection\n```bash\n# Test -w/--workflow:\napr run 1 --dry-run -w myworkflow\napr run 1 --dry-run --workflow myworkflow\n\n# Verify correct workflow loaded\n```\n\n### 7. Output File Handling\n```bash\n# Test existing output file:\ntouch .apr/rounds/default/round_1.md\napr run 1 --dry-run\n\n# Verify: Warning about existing file\n# Interactive: Prompt for overwrite\n# Non-interactive: Proceeds\n```\n\n### 8. Round Number Validation\n```bash\n# Test invalid round numbers:\napr run abc       # Error: must be positive integer\napr run -1        # Error: must be positive integer\napr run 0         # Works (edge case)\napr run 999       # Works\n\n# Test shorthand:\napr 5             # Equivalent to apr run 5\n```\n\n### 9. Verbose Mode\n```bash\n# Test --verbose/-v:\napr run 1 --dry-run --verbose\n\n# Verify verbose output includes:\n- Config loading details\n- Option parsing details\n- File validation details\n```\n\n### 10. Quiet Mode\n```bash\n# Test --quiet/-q:\napr run 1 --dry-run --quiet\n\n# Verify minimal output (errors only)\n```\n\n## Test Implementation\n```bash\n@test \"run dry-run shows correct oracle command\" {\n    setup_test_workflow\n    \n    run apr run 1 --dry-run\n    \n    log_test_output \"$output\"\n    \n    assert_success\n    assert_output --partial \"oracle\"\n    assert_output --partial \"--engine browser\"\n    assert_output --partial \"-m \\\"5.2 Thinking\\\"\"\n    assert_output --partial \"--slug \\\"apr-default-round-1\\\"\"\n}\n\n@test \"run validates round number\" {\n    setup_test_workflow\n    \n    run apr run abc\n    \n    assert_failure\n    assert_output --partial \"must be a positive integer\"\n}\n```\n\n## Acceptance Criteria\n- [ ] dry-run shows complete Oracle command\n- [ ] render outputs prompt correctly\n- [ ] Preflight validation working\n- [ ] --no-preflight skips checks\n- [ ] --include-impl flag working\n- [ ] Workflow selection working\n- [ ] Round number validation correct\n- [ ] Verbose/quiet modes working\n- [ ] All option combinations tested","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:10:13.95116383-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:10:13.95116383-05:00"}
{"id":"automated_plan_reviser_pro-2uc","title":"Unit Tests: Preflight and Validation Functions","description":"# Task: Unit Tests for Preflight and Validation Functions\n\n## Objective\nTest pre-run validation with REAL file system checks.\n\n## Functions to Test\n\n### 1. preflight_check() - Pre-flight Validation\n```bash\n# Test cases with real files:\n# Setup:\nmkdir -p /tmp/apr_test\necho \"# README\" \u003e /tmp/apr_test/README.md\necho \"# SPEC\" \u003e /tmp/apr_test/SPEC.md\necho \"# IMPL\" \u003e /tmp/apr_test/IMPL.md\n\n# Happy path:\n- All required files exist → return 0\n- Oracle available → success message\n\n# File validation:\n- README missing → return 1, error message\n- README not readable (chmod 000) → return 1\n- Spec missing → return 1\n- Spec not readable → return 1\n\n# Implementation file (optional):\n- impl_path provided but missing → return 2 (warning)\n- impl_path provided but not readable → return 2 (warning)\n- impl_path provided and valid → return 0\n\n# Oracle checks:\n- Oracle not available → return 1\n- Oracle version check fails → return 2 (warning)\n\n# Output verification:\n- File sizes displayed correctly\n- Appropriate success/error/warning messages\n```\n\n### 2. run_round() Validation Path\n```bash\n# Test the validation within run_round:\n- Missing workflow config → friendly welcome or error\n- Missing required documents → error\n- include_impl but no impl configured → warning\n- include_impl but impl file missing → warning, continue without\n- Output file exists → prompt for overwrite\n```\n\n### 3. Robot Mode Validation (robot_validate)\n```bash\n# Test cases:\n- Round number missing → error JSON\n- Not initialized → error JSON\n- Workflow not found → error JSON\n- README missing → errors array populated\n- Spec missing → errors array populated\n- Oracle not available → errors array populated\n- Previous round missing → warnings array (not error)\n- All valid → valid:true response\n```\n\n## Test Fixtures Required\n```\ntests/fixtures/documents/\n├── valid_readme.md      # Well-formed README\n├── valid_spec.md        # Well-formed spec\n├── valid_impl.md        # Well-formed implementation\n├── empty.md             # Empty file\n└── binary_file          # Non-text file\n```\n\n## Acceptance Criteria\n- [ ] preflight_check return codes correct\n- [ ] All file checks use real filesystem\n- [ ] Error messages helpful and specific\n- [ ] Warning vs error distinction correct\n- [ ] robot_validate JSON output valid\n- [ ] Verbose logging for debugging","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T20:09:15.467431863-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:09:15.467431863-05:00"}
{"id":"automated_plan_reviser_pro-4nt","title":"Integration Tests: Setup Wizard and Workflow Creation","description":"# Task: Integration Tests for Setup Wizard\n\n## Objective\nTest the complete setup wizard flow with real file creation.\n\n## Test Scenarios\n\n### 1. First-Time Setup (Interactive)\n```bash\n# Use expect or similar for interactive testing\n# Flow:\n1. apr setup (with no existing .apr)\n2. Enter workflow name\n3. Select/enter README path\n4. Select/enter spec path  \n5. Optionally select implementation path\n6. Select GPT model\n7. Verify:\n   - .apr/config.yaml created\n   - .apr/workflows/\u003cname\u003e.yaml created\n   - .apr/rounds/\u003cname\u003e/ directory created\n   - Config contains correct paths\n   - Default workflow set in config.yaml\n\n# Automated version using predefined inputs:\necho -e \"test-workflow\\n\\nREADME.md\\nSPEC.md\\n\\n5.2 Thinking\" | apr setup\n```\n\n### 2. Additional Workflow Setup\n```bash\n# With existing .apr directory:\n1. apr setup\n2. Create second workflow\n3. Verify:\n   - New workflow config created\n   - Default workflow unchanged\n   - Both workflows listed by apr list\n```\n\n### 3. Setup with Implementation Doc\n```bash\n# Flow including implementation:\n1. apr setup\n2. Provide all three document paths\n3. Verify implementation path in config\n```\n\n### 4. Setup Validation\n```bash\n# Test validation during setup:\n- Workflow name with spaces → handled\n- Workflow name with special chars → handled or rejected\n- Non-existent file paths → warning/error\n- Empty workflow name → error\n```\n\n### 5. Gum vs ANSI Modes\n```bash\n# Test both UI modes:\nAPR_NO_GUM=1 apr setup  # Test ANSI fallback\napr setup               # Test gum mode (if available)\n```\n\n## Test Implementation\n```bash\n@test \"setup creates correct directory structure\" {\n    cd \"$TEST_DIR\"\n    mkdir project \u0026\u0026 cd project\n    echo \"# README\" \u003e README.md\n    echo \"# SPEC\" \u003e SPECIFICATION.md\n    \n    # Simulate interactive input\n    run bash -c 'echo -e \"myworkflow\\n\\nREADME.md\\nSPECIFICATION.md\\n\\n5.2 Thinking\" | apr setup'\n    \n    log_test_output \"$output\"\n    \n    assert_success\n    assert [ -d \".apr\" ]\n    assert [ -f \".apr/config.yaml\" ]\n    assert [ -f \".apr/workflows/myworkflow.yaml\" ]\n    assert [ -d \".apr/rounds/myworkflow\" ]\n    \n    # Verify config content\n    assert grep -q \"default_workflow: myworkflow\" .apr/config.yaml\n    assert grep -q \"readme: README.md\" .apr/workflows/myworkflow.yaml\n}\n```\n\n## Test Fixtures\n```\ntests/fixtures/setup/\n├── sample_readme.md    # Sample README for testing\n├── sample_spec.md      # Sample specification\n└── sample_impl.md      # Sample implementation\n```\n\n## Acceptance Criteria\n- [ ] Fresh setup creates all required files/dirs\n- [ ] Additional workflow setup works\n- [ ] Implementation doc optional and works\n- [ ] Both gum and ANSI modes tested\n- [ ] Validation catches invalid inputs\n- [ ] Generated config is valid YAML\n- [ ] Detailed logging of each step","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:09:53.762145144-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:09:53.762145144-05:00"}
{"id":"automated_plan_reviser_pro-ayr","title":"Unit Tests: Utility Functions (version_gt, can_prompt, iso_timestamp)","description":"# Task: Unit Tests for Utility Functions\n\n## Objective\nCreate comprehensive unit tests for all utility functions in APR without using mocks.\n\n## Functions to Test\n\n### 1. version_gt() - Semantic Version Comparison\n```bash\n# Test cases:\n- version_gt \"1.2.0\" \"1.1.0\"  # true (minor bump)\n- version_gt \"2.0.0\" \"1.9.9\"  # true (major bump)\n- version_gt \"1.0.1\" \"1.0.0\"  # true (patch bump)\n- version_gt \"1.0.0\" \"1.0.0\"  # false (equal)\n- version_gt \"1.0.0\" \"1.0.1\"  # false (older)\n- version_gt \"1.10.0\" \"1.9.0\" # true (double digit)\n- version_gt \"0.1.0\" \"0.0.9\"  # true (zero major)\n# Edge cases:\n- Empty strings\n- Invalid version formats\n- Sort -V fallback vs string comparison\n```\n\n### 2. can_prompt() - Interactive Terminal Detection\n```bash\n# Test cases:\n- TTY on stdin and stderr → true\n- No TTY on stdin → false\n- No TTY on stderr → false\n- QUIET_MODE=true → false\n# Real testing approach:\n- Use script(1) or expect to simulate TTY\n- Test with actual pipes/redirects\n```\n\n### 3. iso_timestamp() - ISO8601 Timestamp\n```bash\n# Test cases:\n- Format matches YYYY-MM-DDTHH:MM:SSZ\n- Timezone is UTC (Z suffix)\n- Incrementing calls show time progression\n```\n\n### 4. verbose() - Debug Logging\n```bash\n# Test cases:\n- VERBOSE=false → no output\n- VERBOSE=true → output to stderr\n- Output format includes [apr:verbose] prefix\n```\n\n## Test File: tests/unit/test_utils.bats\n\n## Logging Requirements\nEach test must log:\n- Function name being tested\n- Input values\n- Expected output\n- Actual output\n- Pass/fail with reason\n\n## Acceptance Criteria\n- [ ] version_gt: 10+ test cases covering all scenarios\n- [ ] can_prompt: 5+ test cases with real TTY simulation\n- [ ] iso_timestamp: Format validation tests\n- [ ] verbose: Output control tests\n- [ ] All tests log detailed information\n- [ ] No mocks - real function behavior only","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:07:48.13409656-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:07:48.13409656-05:00"}
{"id":"automated_plan_reviser_pro-de5","title":"Unit Tests: Robot Mode JSON Functions (robot_json, robot_status, robot_workflows, etc.)","description":"# Task: Unit Tests for Robot Mode Functions\n\n## Objective\nTest all robot mode functions that produce JSON output.\n\n## Functions to Test\n\n### 1. robot_json() - JSON Response Builder\n```bash\n# Test cases:\n- ok=true, code=\"ok\" → valid JSON envelope\n- ok=false, code=\"error\" → valid JSON envelope\n- hint parameter included when provided\n- hint parameter omitted when empty\n- meta.v contains VERSION\n- meta.ts is valid ISO8601\n- ROBOT_COMPACT=true → minified output\n- ROBOT_COMPACT=false → pretty-printed\n\n# JSON validation:\n- All outputs parseable by jq\n- No trailing characters\n- Correct escaping of special characters in data\n```\n\n### 2. robot_status() - System Status\n```bash\n# Test cases:\n- Not configured → configured:false\n- Configured → configured:true, default_workflow set\n- Workflows listed in array\n- Oracle available via global → method:\"global\"\n- Oracle available via npx → method:\"npx\"\n- Oracle not available → oracle_available:false\n- Hint provided when not configured\n- Hint provided when oracle missing\n```\n\n### 3. robot_workflows() - Workflow Listing\n```bash\n# Test cases:\n- No workflows dir → not_configured error\n- Empty workflows dir → empty array\n- Multiple workflows → array with name/description\n- Description extraction from YAML\n```\n\n### 4. robot_init() - Initialization\n```bash\n# Test cases:\n- Already initialized → created:false, existed:true\n- Fresh init → created:true, existed:false\n- Directory creation failure → init_failed error\n- Config write failure → init_failed error\n```\n\n### 5. robot_validate() - Pre-run Validation\n```bash\n# Test cases:\n- Valid state → valid:true, empty errors\n- Round number missing → errors array\n- Workflow missing → errors array\n- Files missing → errors array\n- Previous round missing → warnings array (not errors)\n- JSON structure correct for all cases\n```\n\n### 6. robot_run() - Execution\n```bash\n# Test cases (dry-run/validation only, not actual Oracle):\n- Missing round → missing_argument error\n- Non-numeric round → invalid_argument error\n- Workflow not found → not_found error\n- Files not found → not_found error\n- Oracle not available → dependency_missing error\n- Valid request → returns slug, pid, output_file\n\n# Note: Actual Oracle execution tested in integration\n```\n\n### 7. robot_history() - Round Listing\n```bash\n# Test cases:\n- No rounds dir → not_found error\n- Empty rounds dir → empty rounds array\n- Multiple rounds → array with round, file, size, modified\n- JSON numbers (not strings) for round, size, modified\n- tonumber conversion handles edge cases\n```\n\n### 8. robot_help() - Help Output\n```bash\n# Test cases:\n- Valid JSON structure\n- All commands documented\n- All options documented\n- Examples included\n```\n\n## Acceptance Criteria\n- [ ] All robot_* functions produce valid JSON\n- [ ] Error responses use correct codes\n- [ ] Hints are helpful for error resolution\n- [ ] jq can parse all outputs\n- [ ] ROBOT_COMPACT mode works\n- [ ] No bash errors leak into JSON output","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T20:09:35.23404558-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:09:35.23404558-05:00"}
{"id":"automated_plan_reviser_pro-hqt","title":"Set up BATS testing framework and directory structure","description":"# Task: Set up BATS Testing Framework\n\n## Objective\nInstall and configure BATS (Bash Automated Testing System) for APR unit and integration testing.\n\n## Deliverables\n\n### 1. Directory Structure\n```\ntests/\n├── unit/                    # Unit tests for individual functions\n│   ├── test_utils.bats      # Utility function tests\n│   ├── test_config.bats     # Config parsing tests\n│   ├── test_output.bats     # Output function tests\n│   └── test_lock.bats       # Lock mechanism tests\n├── integration/             # Command-level tests\n│   ├── test_setup.bats      # Setup wizard tests\n│   ├── test_run.bats        # Run command tests\n│   ├── test_commands.bats   # Other commands\n│   └── test_robot.bats      # Robot mode tests\n├── e2e/                     # End-to-end workflow tests\n│   ├── test_full_workflow.bats\n│   └── test_error_recovery.bats\n├── fixtures/                # Test data\n│   ├── configs/             # Sample workflow configs\n│   ├── documents/           # Sample README, spec files\n│   └── outputs/             # Expected outputs\n├── helpers/                 # Test utilities\n│   ├── test_helper.bash     # Common setup/teardown\n│   ├── logging.bash         # Detailed test logging\n│   └── assertions.bash      # Custom assertions\n└── run_tests.sh             # Test runner with logging\n```\n\n### 2. BATS Installation\n- Add bats-core as submodule or document installation\n- Include bats-support and bats-assert helpers\n- Verify Bash 4.0+ compatibility\n\n### 3. Test Helper Functions\n```bash\n# test_helper.bash should include:\n- setup_test_environment()    # Create isolated temp dir\n- teardown_test_environment() # Cleanup\n- load_apr_functions()        # Source apr for unit testing\n- log_test_step()            # Detailed logging\n- assert_file_contains()     # Custom assertions\n- assert_exit_code()         # Exit code verification\n```\n\n### 4. Logging Infrastructure\n- Each test logs: test name, inputs, expected output, actual output\n- Log file per test run with timestamp\n- Summary report at end\n\n## Acceptance Criteria\n- [ ] BATS framework installed and working\n- [ ] Directory structure created\n- [ ] test_helper.bash with setup/teardown\n- [ ] logging.bash with detailed logging\n- [ ] run_tests.sh executes all tests\n- [ ] Sample test passes","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:07:33.597862458-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:07:33.597862458-05:00"}
{"id":"automated_plan_reviser_pro-ifg","title":"Unit Tests: Output Functions (print_*, spin, confirm, choose, input)","description":"# Task: Unit Tests for Output Functions\n\n## Objective\nTest all terminal output functions with both gum and ANSI fallback modes.\n\n## Functions to Test\n\n### 1. print_banner() - Main Banner Display\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum output\n- GUM_AVAILABLE=false → ANSI output\n- QUIET_MODE=true → no output\n- Terminal width adaptation (60 max, 40 min)\n- Version number displayed correctly\n```\n\n### 2. print_* Functions (success, error, warning, info, dim, header)\n```bash\n# For each function test:\n- Output goes to stderr\n- GUM_AVAILABLE=true → gum styling\n- GUM_AVAILABLE=false → ANSI codes\n- QUIET_MODE suppresses appropriate functions\n- Correct emoji/prefix (✓, ✗, ⚠, ℹ)\n\n# Note: print_error should NOT be suppressed by QUIET_MODE\n```\n\n### 3. print_step() - Step Progress Display\n```bash\n# Test cases:\n- Normal step: [1/5] message\n- Optional step: [Optional] message\n- GUM vs ANSI modes\n- QUIET_MODE suppression\n```\n\n### 4. spin() - Spinner Wrapper\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum spin\n- GUM_AVAILABLE=false → simple message\n- QUIET_MODE → no visual, command still runs\n- Command exit code preserved\n```\n\n### 5. confirm() - User Confirmation\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum confirm\n- ANSI fallback with [Y/n] or [y/N] prompts\n- Default true/false behavior\n- Non-interactive mode returns default\n- Case insensitivity (Y/y/YES/yes)\n```\n\n### 6. choose() - Selection Menu\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum choose\n- ANSI fallback with numbered list\n- Default to first option on invalid input\n- Output goes to stdout (selection result)\n```\n\n### 7. input() - Text Input\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum input\n- ANSI fallback with read\n- Default value handling\n- Empty input handling\n```\n\n### 8. file_picker() - File Selection\n```bash\n# Test cases:\n- GUM_AVAILABLE=true → gum file (no --all flag)\n- ANSI fallback prompts for path\n- Hidden files excluded in gum mode\n```\n\n## Testing Approach\n- Capture stderr for output validation\n- Use NO_COLOR=1 for deterministic ANSI output\n- Test both GUM_AVAILABLE=true and false paths\n\n## Acceptance Criteria\n- [ ] All print_* functions tested in both modes\n- [ ] Interactive functions tested with simulated input\n- [ ] QUIET_MODE behavior verified for each function\n- [ ] stderr vs stdout verified for each function\n- [ ] Detailed logs showing actual vs expected output","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T20:08:20.763981354-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:08:20.763981354-05:00"}
{"id":"automated_plan_reviser_pro-it2","title":"Integration Tests: Robot Mode Commands (JSON API)","description":"# Task: Integration Tests for Robot Mode\n\n## Objective\nTest the complete robot mode JSON API for coding agent integration.\n\n## Command Tests\n\n### 1. apr robot status\n```bash\n# Not configured:\nrun apr robot status\n# Verify:\n- Valid JSON\n- configured: false\n- Hint about initialization\n\n# Configured:\n# Setup: Initialize .apr\nrun apr robot status\n# Verify:\n- configured: true\n- default_workflow set\n- workflows array populated\n- oracle_available status correct\n```\n\n### 2. apr robot workflows\n```bash\n# No workflows:\nrun apr robot workflows\n# Verify: not_configured error\n\n# With workflows:\nrun apr robot workflows\n# Verify:\n- ok: true\n- workflows array with name/description\n- Valid JSON\n```\n\n### 3. apr robot init\n```bash\n# Fresh init:\nrun apr robot init\n# Verify:\n- ok: true\n- created: true\n- .apr directory created\n\n# Already initialized:\nrun apr robot init\n# Verify:\n- ok: true\n- created: false\n- existed: true\n```\n\n### 4. apr robot validate \u003cround\u003e\n```bash\n# Valid state:\n# Setup: Complete workflow with files\nrun apr robot validate 1\n# Verify:\n- ok: true\n- valid: true\n- empty errors array\n\n# Invalid state:\n# Setup: Missing required files\nrun apr robot validate 1\n# Verify:\n- ok: false\n- valid: false\n- errors array populated\n```\n\n### 5. apr robot run \u003cround\u003e\n```bash\n# Note: Actually starts Oracle in background\n# Test validation only (don't wait for Oracle):\n\n# Missing round:\nrun apr robot run\n# Verify: missing_argument error\n\n# Invalid round:\nrun apr robot run abc\n# Verify: invalid_argument error\n\n# Workflow options:\nrun apr robot run 1 -w myworkflow\n# Verify: Correct workflow used\n\n# Include impl:\nrun apr robot run 1 --include-impl\n# Verify: include_impl in response\n```\n\n### 6. apr robot history\n```bash\n# No rounds:\nrun apr robot history\n# Verify: not_found error\n\n# With rounds:\nrun apr robot history\n# Verify:\n- ok: true\n- count correct\n- rounds array with round, file, size, modified\n- All numbers are JSON numbers (not strings)\n```\n\n### 7. apr robot help\n```bash\nrun apr robot help\n# Verify:\n- Valid JSON\n- All commands documented\n- Examples included\n```\n\n### 8. Robot Mode Options\n```bash\n# Compact output:\nrun apr robot status --compact\n# Verify: Minified JSON (no newlines/indentation)\n\n# Workflow selection:\nrun apr robot history -w myworkflow\n# Verify: Correct workflow used\n\n# Error output:\nrun apr robot unknown_command\n# Verify: unknown_command error, valid JSON\n```\n\n### 9. jq Requirement\n```bash\n# Test without jq (mock unavailable):\n# Verify: Appropriate error message in JSON-like format\n```\n\n## JSON Validation\n```bash\n# All tests should verify:\nvalidate_robot_json() {\n    local output=\"$1\"\n    \n    # Must be valid JSON\n    echo \"$output\" | jq . \u003e /dev/null || fail \"Invalid JSON\"\n    \n    # Must have envelope structure\n    echo \"$output\" | jq -e '.ok != null' \u003e /dev/null || fail \"Missing .ok\"\n    echo \"$output\" | jq -e '.code != null' \u003e /dev/null || fail \"Missing .code\"\n    echo \"$output\" | jq -e '.data != null' \u003e /dev/null || fail \"Missing .data\"\n    echo \"$output\" | jq -e '.meta.v != null' \u003e /dev/null || fail \"Missing .meta.v\"\n    echo \"$output\" | jq -e '.meta.ts != null' \u003e /dev/null || fail \"Missing .meta.ts\"\n}\n```\n\n## Acceptance Criteria\n- [ ] All robot commands produce valid JSON\n- [ ] Error codes meaningful and documented\n- [ ] Hints helpful for error resolution\n- [ ] --compact mode works\n- [ ] -w workflow selection works everywhere\n- [ ] No stderr pollution of JSON output\n- [ ] jq can parse all outputs\n- [ ] Coding agents can parse and use output","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:10:54.188078289-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:10:54.188078289-05:00"}
{"id":"automated_plan_reviser_pro-iw3","title":"Integration Tests: Management Commands (list, history, show, status, attach)","description":"# Task: Integration Tests for Management Commands\n\n## Objective\nTest all workflow management commands with real data.\n\n## Commands to Test\n\n### 1. apr list - List Workflows\n```bash\n# No workflows:\napr list\n# Verify: \"No workflows configured yet\" message\n\n# With workflows:\n# Setup: Create multiple workflows\napr list\n# Verify:\n- All workflows listed\n- Descriptions shown\n- Default workflow marked\n- Both gum and ANSI output tested\n```\n\n### 2. apr history - Revision History\n```bash\n# No rounds:\napr history\n# Verify: \"No rounds recorded yet\" message\n\n# With rounds:\n# Setup: Create some round output files\napr history\n# Verify:\n- All rounds listed\n- File sizes shown\n- Dates shown (cross-platform)\n- Latest round marked\n- Preview of first line shown\n\n# Workflow selection:\napr history -w myworkflow\n# Verify: Shows history for specific workflow\n```\n\n### 3. apr show \u003cround\u003e - View Round Output\n```bash\n# Round exists:\napr show 1\n# Verify: Content displayed (bat/less/cat fallback)\n\n# Round doesn't exist:\napr show 99\n# Verify: Error message with helpful hint\n\n# Workflow selection:\napr show 1 -w myworkflow\n# Verify: Shows correct workflow's round\n```\n\n### 4. apr status - Oracle Session Status\n```bash\n# Note: This calls Oracle, so may need mock or skip in CI\n\n# Test command execution:\napr status\n# Verify: Oracle status command executed\n\n# Test --hours option:\napr status --hours 24\n# Verify: Correct hours passed to Oracle\n```\n\n### 5. apr attach \u003csession\u003e - Attach to Session\n```bash\n# Note: This calls Oracle, so may need mock or skip in CI\n\n# Test command execution:\napr attach apr-default-round-1\n# Verify: Oracle session command executed with --render\n```\n\n### 6. apr diff \u003cN\u003e [M] - Compare Rounds\n```bash\n# Setup: Create round files with different content\n\n# Single round (compare with previous):\napr diff 3\n# Verify: Compares round 3 with round 2\n\n# Two rounds:\napr diff 2 5\n# Verify: Compares round 2 with round 5\n\n# Round 1 alone:\napr diff 1\n# Verify: Error (no previous round)\n\n# Missing round:\napr diff 99\n# Verify: Error message\n\n# Tool detection:\n# With delta: Uses delta\n# Without delta: Falls back to diff\n```\n\n### 7. apr integrate \u003cround\u003e - Generate Integration Prompt\n```bash\n# Test basic:\napr integrate 3\n# Verify: Integration prompt generated to stdout\n\n# Test --copy:\napr integrate 3 --copy\n# Verify: Copied to clipboard (if available)\n\n# Test --output:\napr integrate 3 --output /tmp/prompt.md\n# Verify: Written to file\n```\n\n### 8. apr stats - Round Analytics\n```bash\n# No rounds:\napr stats\n# Verify: Appropriate message\n\n# With rounds:\napr stats\n# Verify:\n- Round count\n- Average size\n- Trend signal (if enough rounds)\n- Table of rounds with sizes/dates\n```\n\n## Test Fixtures\n```\ntests/fixtures/rounds/\n├── round_1.md    # Small round output\n├── round_2.md    # Medium round output\n├── round_3.md    # Large round output\n└── round_4.md    # Different content for diff testing\n```\n\n## Acceptance Criteria\n- [ ] All management commands tested\n- [ ] Empty state handling correct\n- [ ] Workflow selection working\n- [ ] Error messages helpful\n- [ ] Both gum and ANSI modes tested\n- [ ] Cross-platform compatibility verified","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T20:10:33.908224761-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:10:33.908224761-05:00"}
{"id":"automated_plan_reviser_pro-ixw","title":"Unit Tests: Lock Mechanism (acquire_lock, release_lock, cleanup_temp)","description":"# Task: Unit Tests for Lock and Cleanup Mechanisms\n\n## Objective\nTest concurrent execution prevention and cleanup mechanisms with REAL locking behavior.\n\n## Functions to Test\n\n### 1. acquire_lock() - Lock Acquisition\n```bash\n# Test cases with flock available:\n- First process acquires lock → returns 0\n- Second process fails to acquire → returns 1\n- Lock file created in correct location\n- PID written to lock file\n\n# Test cases with flock fallback:\n- APR_LOCK_FD not used (file-based only)\n- Stale lock detection (PID not running)\n- Stale lock cleanup before acquisition\n\n# Concurrent test:\n- Start background process holding lock\n- Attempt to acquire from foreground\n- Verify failure\n- Kill background, verify reacquisition works\n\n# Edge cases:\n- Lock directory doesn't exist → created\n- Lock file permissions\n- Workflow/round naming in lock file path\n```\n\n### 2. release_lock() - Lock Release\n```bash\n# Test cases:\n- Release after flock acquisition → FD closed\n- Release after file-based lock → file deleted\n- Release without acquisition → no error\n- APR_LOCK_FILE and APR_LOCK_FD cleared\n- Idempotent (safe to call multiple times)\n```\n\n### 3. cleanup_temp() - Exit Cleanup\n```bash\n# Test cases:\n- APR_TEMP_DIR set and exists → removed\n- APR_TEMP_DIR empty → no error\n- APR_TEMP_DIR doesn't exist → no error\n- Lock released during cleanup\n- Trap triggers on EXIT\n- Trap triggers on INT (Ctrl+C simulation)\n- Trap triggers on TERM\n\n# Real cleanup test:\n- Create temp dir\n- Set APR_TEMP_DIR\n- Exit script\n- Verify temp dir removed\n```\n\n### 4. Trap Integration\n```bash\n# Test the trap registration:\n- trap cleanup_temp EXIT INT TERM\n- Verify cleanup runs on normal exit\n- Verify cleanup runs on error exit (set -e trigger)\n- Verify cleanup runs on signal\n```\n\n## Testing Approach\n```bash\n# Example: Concurrent lock test\ntest_concurrent_lock() {\n    local lock_dir=$(mktemp -d)\n    export CONFIG_DIR=\"$lock_dir\"\n    \n    # Background process holds lock\n    (\n        source apr\n        acquire_lock \"test\" \"1\"\n        sleep 10\n    ) \u0026\n    local bg_pid=$!\n    sleep 0.5  # Let it acquire\n    \n    # Foreground should fail\n    source apr\n    if acquire_lock \"test\" \"1\"; then\n        fail \"Should not acquire lock\"\n    fi\n    \n    # Cleanup\n    kill $bg_pid 2\u003e/dev/null\n    wait $bg_pid 2\u003e/dev/null\n    rm -rf \"$lock_dir\"\n}\n```\n\n## Acceptance Criteria\n- [ ] acquire_lock tested with flock and fallback\n- [ ] Concurrent lock prevention verified\n- [ ] Stale lock detection works\n- [ ] release_lock properly cleans up\n- [ ] cleanup_temp removes temp directories\n- [ ] Trap fires on all exit conditions\n- [ ] No race conditions in tests","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:08:40.196397234-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:08:40.196397234-05:00"}
{"id":"automated_plan_reviser_pro-ufc","title":"Testing Infrastructure: Complete test coverage for APR","description":"# APR Testing Infrastructure Epic\n\n## Overview\nImplement comprehensive testing for APR including unit tests, integration tests, and end-to-end tests. All tests should use REAL behavior (no mocks/fakes) and include detailed logging.\n\n## Philosophy\n- **No mocks**: Test actual functions with real inputs/outputs\n- **Isolated environments**: Use temp directories for each test\n- **Detailed logging**: Every test logs what it's doing for debugging\n- **Deterministic**: Tests should be reproducible\n\n## Test Categories\n1. **Unit Tests**: Individual function testing with BATS\n2. **Integration Tests**: Command-level testing with real configs\n3. **E2E Tests**: Full workflow testing with logging\n\n## Success Criteria\n- 100% function coverage for utility functions\n- All commands have integration tests\n- Full workflow E2E tests pass\n- CI integration working","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-12T20:07:17.440688231-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:07:17.440688231-05:00"}
{"id":"automated_plan_reviser_pro-uos","title":"Unit Tests: Config Parsing Functions (get_config_value, get_yaml_block, load_prompt_template)","description":"# Task: Unit Tests for Config Parsing Functions\n\n## Objective\nTest all YAML/config parsing functions with real config files (no mocks).\n\n## Functions to Test\n\n### 1. get_config_value() - Simple YAML Value Extraction\n```bash\n# Test with real YAML files:\n# fixtures/configs/simple.yaml:\n#   name: test-workflow\n#   description: \"A test workflow\"\n#   documents:\n#     readme: README.md\n#     spec: SPEC.md\n\n# Test cases:\n- Top-level key extraction (name, description)\n- Nested key extraction (readme under documents)\n- Key with quoted value\n- Key with spaces in value\n- Missing key → empty string\n- Empty value → empty string\n- Key with colon in value\n- Multiple colons in line\n```\n\n### 2. get_yaml_block() - Block Scalar Extraction\n```bash\n# Test with real YAML containing block scalars:\n# fixtures/configs/with_template.yaml:\n#   template: |\n#     First line\n#     Second line\n#     Third line\n\n# Test cases:\n- Literal block scalar (|)\n- Folded block scalar (\u003e)\n- Block with indicators (|-, |+)\n- Nested content with varying indentation\n- Block ending at next top-level key\n- Block ending at EOF\n- Empty block\n- Block with special characters\n```\n\n### 3. load_prompt_template() - Template Loading\n```bash\n# Test cases:\n- Load 'template' block\n- Load 'template_with_impl' when include_impl=true\n- Fallback to 'template' when no template_with_impl\n- Missing template → empty\n- Empty config file\n- Non-existent file handling\n```\n\n### 4. load_config() - Config File Resolution\n```bash\n# Test cases:\n- Load workflow-specific config\n- Fallback to global config.yaml\n- Return error when no config exists\n```\n\n## Test Fixtures Required\n```\ntests/fixtures/configs/\n├── simple.yaml              # Basic key-value pairs\n├── nested.yaml              # Nested YAML structure\n├── with_template.yaml       # Contains template blocks\n├── with_template_impl.yaml  # Contains both template types\n├── special_chars.yaml       # Values with colons, quotes\n├── empty.yaml               # Empty file\n└── malformed.yaml           # Invalid YAML for error testing\n```\n\n## Acceptance Criteria\n- [ ] get_config_value: 15+ test cases\n- [ ] get_yaml_block: 10+ test cases\n- [ ] load_prompt_template: 8+ test cases\n- [ ] All tests use real fixture files\n- [ ] Edge cases documented and tested\n- [ ] Detailed logging for each test","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:08:03.678553238-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:08:03.678553238-05:00"}
{"id":"automated_plan_reviser_pro-vv1","title":"E2E Tests: Full Workflow (setup → run → history → show)","description":"# Task: End-to-End Full Workflow Tests\n\n## Objective\nTest complete user journeys from start to finish with detailed logging.\n\n## E2E Scenario 1: New User Complete Flow\n```bash\n# Logging: Each step logs timestamp, action, result\n\n# Step 1: Fresh start\nlog \"Starting E2E: New user flow\"\ncd $(mktemp -d)\ngit init\necho \"# My Project\" \u003e README.md\necho \"# Specification\" \u003e SPECIFICATION.md\nlog \"Created project directory with README and SPEC\"\n\n# Step 2: First-run experience\nlog \"Testing first-run experience (no args)\"\nrun apr\nassert_output --partial \"WELCOME TO APR\"\nassert_output --partial \"apr setup\"\nlog \"First-run welcome displayed correctly\"\n\n# Step 3: Setup wizard\nlog \"Running setup wizard\"\nrun bash -c 'echo -e \"myproject\\nProject workflow\\nREADME.md\\nSPECIFICATION.md\\n\\n5.2 Thinking\" | apr setup'\nassert_success\nassert [ -d \".apr\" ]\nlog \"Setup completed: $(ls -la .apr/)\"\n\n# Step 4: Verify configuration\nlog \"Verifying configuration files\"\nassert [ -f \".apr/config.yaml\" ]\nassert [ -f \".apr/workflows/myproject.yaml\" ]\nrun cat .apr/workflows/myproject.yaml\nlog \"Workflow config: $output\"\n\n# Step 5: List workflows\nlog \"Listing workflows\"\nrun apr list\nassert_output --partial \"myproject\"\nassert_output --partial \"(default)\"\nlog \"Workflow listing correct\"\n\n# Step 6: Dry run\nlog \"Testing dry run\"\nrun apr run 1 --dry-run\nassert_success\nassert_output --partial \"oracle\"\nassert_output --partial \"--slug\"\nlog \"Dry run output: $output\"\n\n# Step 7: Create mock round output\nlog \"Creating mock round output\"\nmkdir -p .apr/rounds/myproject\necho \"# Round 1 Analysis\\n\\nThis is a test output.\" \u003e .apr/rounds/myproject/round_1.md\nlog \"Mock round created\"\n\n# Step 8: View history\nlog \"Checking history\"\nrun apr history\nassert_output --partial \"Round 1\"\nassert_output --partial \"(latest)\"\nlog \"History display correct\"\n\n# Step 9: Show round\nlog \"Showing round 1\"\nrun apr show 1\nassert_output --partial \"Round 1 Analysis\"\nlog \"Round content displayed correctly\"\n\n# Step 10: Run round 2 (dry-run)\nlog \"Dry run for round 2\"\nrun apr run 2 --dry-run\nassert_output --partial \"round-2\"\nlog \"Round 2 dry run successful\"\n\nlog \"E2E Scenario 1 PASSED\"\n```\n\n## E2E Scenario 2: Multi-Workflow Management\n```bash\nlog \"Starting E2E: Multi-workflow management\"\n\n# Setup first workflow\nlog \"Creating first workflow\"\n# ... setup steps ...\n\n# Setup second workflow\nlog \"Creating second workflow\"\n# ... setup with different name ...\n\n# List shows both\nlog \"Verifying both workflows listed\"\nrun apr list\nassert_output --partial \"workflow1\"\nassert_output --partial \"workflow2\"\n\n# Run on specific workflow\nlog \"Running on workflow2\"\nrun apr run 1 --dry-run -w workflow2\nassert_output --partial \"workflow2\"\n\n# History for specific workflow\nlog \"History for workflow1\"\nrun apr history -w workflow1\n# ... assertions ...\n\nlog \"E2E Scenario 2 PASSED\"\n```\n\n## E2E Scenario 3: Implementation Document Flow\n```bash\nlog \"Starting E2E: Implementation document flow\"\n\n# Setup with implementation\necho \"# Implementation\" \u003e IMPLEMENTATION.md\n# Setup workflow including impl\n\n# Run with --include-impl\nlog \"Running with implementation\"\nrun apr run 1 --dry-run --include-impl\nassert_output --partial \"with-impl\"\nassert_output --partial \"IMPLEMENTATION.md\"\n\nlog \"E2E Scenario 3 PASSED\"\n```\n\n## E2E Scenario 4: Robot Mode Workflow\n```bash\nlog \"Starting E2E: Robot mode workflow\"\n\n# Initialize\nlog \"Initializing via robot mode\"\nrun apr robot init\nassert_success\nresponse=$(echo \"$output\" | jq -r '.ok')\nassert_equal \"$response\" \"true\"\n\n# Status check\nlog \"Status check\"\nrun apr robot status\nconfigured=$(echo \"$output\" | jq -r '.data.configured')\nassert_equal \"$configured\" \"true\"\n\n# Validate before run\nlog \"Validating round 1\"\nrun apr robot validate 1\nvalid=$(echo \"$output\" | jq -r '.data.valid')\n# May be false if files missing - that's expected\n\n# History (empty initially)\nlog \"Checking empty history\"\nrun apr robot history\ncode=$(echo \"$output\" | jq -r '.code')\nassert_equal \"$code\" \"not_found\"\n\nlog \"E2E Scenario 4 PASSED\"\n```\n\n## Logging Infrastructure\n```bash\n# Every E2E test uses comprehensive logging:\nLOG_FILE=\"$TEST_DIR/e2e_$(date +%Y%m%d_%H%M%S).log\"\n\nlog() {\n    local timestamp=$(date '+%Y-%m-%d %H:%M:%S.%3N')\n    echo \"[$timestamp] $*\" | tee -a \"$LOG_FILE\"\n}\n\nlog_section() {\n    log \"==========================================\"\n    log \"$*\"\n    log \"==========================================\"\n}\n\nlog_command() {\n    log \"COMMAND: $*\"\n}\n\nlog_output() {\n    log \"OUTPUT:\"\n    echo \"$*\" | sed 's/^/  /' \u003e\u003e \"$LOG_FILE\"\n}\n```\n\n## Acceptance Criteria\n- [ ] Full new-user workflow passes\n- [ ] Multi-workflow management passes\n- [ ] Implementation document flow passes\n- [ ] Robot mode workflow passes\n- [ ] Detailed logs for debugging\n- [ ] Logs include timestamps\n- [ ] Failure points clearly identified in logs\n- [ ] Tests run in isolated directories","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T20:11:19.859589866-05:00","created_by":"ubuntu","updated_at":"2026-01-12T20:11:19.859589866-05:00"}
